{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T14:54:59.556924Z",
     "start_time": "2025-03-15T14:54:59.274176Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from torchvision import transforms as T\n",
    "from torchmetrics.image import StructuralSimilarityIndexMeasure\n",
    "import torch.utils.dlpack\n",
    "import cupy as cp\n",
    "from cuml import UMAP\n",
    "from cuml.cluster import KMeans, HDBSCAN\n",
    "from cuml.decomposition import IncrementalPCA\n",
    "from cuml.manifold import TSNE\n",
    "from cuml.metrics.cluster import silhouette_score\n",
    "\n",
    "from model.models import *\n",
    "from dataset.dataset import (\n",
    "    device_checker,\n",
    "    load_original_datasets,\n",
    "    denormalize_tensor,\n",
    "    denorm_mean_tensor,\n",
    "    denorm_std_tensor,\n",
    "    create_background_mask_otsu\n",
    ")"
   ],
   "id": "28a2be1ca6eb6a6d",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cupy'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[61], line 11\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorchmetrics\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mimage\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m StructuralSimilarityIndexMeasure\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdlpack\u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mcupy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mcp\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mcuml\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m UMAP\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mcuml\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcluster\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m KMeans, HDBSCAN\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'cupy'"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T14:19:26.845053Z",
     "start_time": "2025-03-15T14:19:23.349941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#root_dir = '/projects/dsci410_510/gz_hubble'\n",
    "root_dir = '../data/gz_hubble'\n",
    "weights_path = '../model/saved_models/CustomAutoencoder.pt'\n",
    "\n",
    "# Initialize\n",
    "device = device_checker()\n",
    "dataset_dir = '../data/gz_hubble'  # UPDATE THIS\n",
    "full_catalog, _, og_test_catalog = load_original_datasets(dataset_dir, return_og_catalogs=True)"
   ],
   "id": "e1ce9a4b371fc614",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using device: mps\n",
      "\n",
      "Loading original datasets from file...\n",
      "Saved the new combined catalog to: ../data/gz_hubble/full_catalog.parquet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'# Create dataloaders\\nsingle_view_transform, double_view_transform = create_transforms()\\n_, _, test_loader, _, _, test_dataset = get_data_loaders(\\n    full_catalog,\\n    double_view_transform,\\n    num_workers=0,\\n    prefetch_factor=0,\\n)'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T14:25:08.855474Z",
     "start_time": "2025-03-15T14:24:25.920439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "images_dir = os.path.join(root_dir, \"images\")\n",
    "image_paths = og_test_catalog['file_loc'].tolist()\n",
    "total_images = len(image_paths)\n",
    "\n",
    "LATENT_DIM = 32  # Match up with value used in model training\n",
    "IMAGE_LOAD_BATCH_SIZE = 256\n",
    "\n",
    "# Load model and weights\n",
    "unet_model = UNetAutoencoder()\n",
    "custom_model = CustomAutoencoder(\n",
    "    activation_type='prelu',\n",
    "    latent_dim=32\n",
    ")\n",
    "model = custom_model  # SET MODEL HERE\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(weights_path, map_location=device, weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "# Define transforms\n",
    "mean = [0.0441, 0.0464, 0.0484]\n",
    "std = [0.0712, 0.0726, 0.0713]\n",
    "transform = T.Compose([\n",
    "    T.Resize((224, 224)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "# Initialize SSIM score metric\n",
    "ssim_metric = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)\n",
    "ssim_metric.reset()\n",
    "ssim_scores = []\n",
    "\n",
    "# Pre-allocate a single large tensor to hold all latent vectors\n",
    "all_latents = torch.empty(total_images, LATENT_DIM, device=device)\n",
    "start_idx = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, total_images, IMAGE_LOAD_BATCH_SIZE)):\n",
    "        batch_files = image_paths[i: i + IMAGE_LOAD_BATCH_SIZE]\n",
    "        batch_tensors = []\n",
    "\n",
    "        for f in batch_files:\n",
    "            with Image.open(f) as pil_img:\n",
    "                pil_img = pil_img.convert(\"RGB\")\n",
    "                t_img = transform(pil_img)\n",
    "            batch_tensors.append(t_img)\n",
    "\n",
    "        batch_input = torch.stack(batch_tensors, dim=0).to(device)\n",
    "\n",
    "        # Obtain reconstruction and latent vectors\n",
    "        recon, latents = model(batch_input, return_latent=True)\n",
    "\n",
    "        # Store latent representations\n",
    "        end_idx = start_idx + latents.shape[0]\n",
    "        all_latents[start_idx:end_idx] = latents\n",
    "        start_idx = end_idx\n",
    "\n",
    "        # Denormalize inputs and reconstructions\n",
    "        batch_input_denorm = denormalize_tensor(batch_input)\n",
    "        recon_denorm = denormalize_tensor(recon)\n",
    "\n",
    "        # Create foreground/galaxy masks for each image in batch\n",
    "        galaxy_masks = []\n",
    "        for img in batch_input_denorm:\n",
    "            background_mask = create_background_mask_otsu(img)\n",
    "            galaxy_mask = 1 - background_mask\n",
    "            galaxy_masks.append(galaxy_mask)\n",
    "        galaxy_mask_tensor = torch.stack(galaxy_masks, dim=0).unsqueeze(1).to(device)\n",
    "\n",
    "        # Apply galaxy mask to both target and reconstruction\n",
    "        input_galaxy = batch_input_denorm * galaxy_mask_tensor\n",
    "        recon_galaxy = recon_denorm * galaxy_mask_tensor\n",
    "\n",
    "        # Compute SSIM only for galaxy region\n",
    "        batch_ssim = ssim_metric(recon_galaxy, input_galaxy)\n",
    "        ssim_scores.append(batch_ssim.item())\n",
    "\n",
    "\n",
    "# Compute overall average SSIM across all batches\n",
    "avg_ssim = sum(ssim_scores) / len(ssim_scores)\n",
    "print(\"Average SSIM on test set galaxy regions:\", np.round(avg_ssim, 3))\n",
    "\n",
    "# Zero-copy conversion via DLPack\n",
    "latents_dlpack = torch.utils.dlpack.to_dlpack(all_latents)\n",
    "all_latents_cp = cp.from_dlpack(latents_dlpack)\n",
    "print(\"Cupy array shape:\", all_latents_cp.shape)"
   ],
   "id": "790648b963516111",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:42<00:00,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latents shape: torch.Size([2000, 32])\n",
      "Average SSIM on test set galaxy regions: 0.936\n",
      "NumPy array shape: (2000, 32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T14:25:47.285477Z",
     "start_time": "2025-03-15T14:25:44.625048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# -----\n",
    "# PCA\n",
    "# -----\n",
    "\n",
    "pca_components = 16\n",
    "batch_size = 256\n",
    "\n",
    "ipca = IncrementalPCA(n_components=pca_components, batch_size=batch_size, verbose=True)\n",
    "\n",
    "num_samples = all_latents_cp.shape[0]\n",
    "i = 0\n",
    "while i < num_samples:\n",
    "    end = min(i + batch_size, num_samples)\n",
    "    current_batch = all_latents_cp[i:end]\n",
    "\n",
    "    # Merge if the final batch is smaller than the PCA dimension\n",
    "    if (end - i) < pca_components and i > 0:\n",
    "        prev_start = i - batch_size\n",
    "        merged_batch = np.concatenate([all_latents_cp[prev_start:i], current_batch], axis=0)\n",
    "        batch_gpu = cp.asarray(merged_batch, dtype=cp.float32)\n",
    "        ipca.partial_fit(batch_gpu)\n",
    "    else:\n",
    "        batch_gpu = cp.asarray(current_batch, dtype=cp.float32)\n",
    "        ipca.partial_fit(batch_gpu)\n",
    "\n",
    "    i += batch_size\n",
    "\n",
    "# Transform the entire dataset into PCA space on GPU\n",
    "data_pca_gpu = cp.asarray(all_latents_cp, dtype=cp.float32)\n",
    "all_latents = ipca.transform(data_pca_gpu)"
   ],
   "id": "e2f0c1648c8ae145",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[31], line 23\u001B[0m\n\u001B[1;32m     21\u001B[0m     ipca\u001B[38;5;241m.\u001B[39mpartial_fit(batch_gpu)\n\u001B[1;32m     22\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 23\u001B[0m     batch_gpu \u001B[38;5;241m=\u001B[39m \u001B[43mcp\u001B[49m\u001B[38;5;241m.\u001B[39masarray(current_batch, dtype\u001B[38;5;241m=\u001B[39mcp\u001B[38;5;241m.\u001B[39mfloat32)\n\u001B[1;32m     24\u001B[0m     ipca\u001B[38;5;241m.\u001B[39mpartial_fit(batch_gpu)\n\u001B[1;32m     26\u001B[0m i \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m batch_size\n",
      "\u001B[0;31mNameError\u001B[0m: name 'cp' is not defined"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convert latent vectors to GPU memory directly\n",
    "all_latents = cp.asarray(all_latents_cp, dtype=cp.float32)\n",
    "\n",
    "# -------------------------\n",
    "# HDBSCAN Clustering\n",
    "# -------------------------\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=25\n",
    ")\n",
    "cluster_labels = hdbscan_model.fit_predict(all_latents)\n",
    "\n",
    "unique_labels_cp, counts_cp = cp.unique(cluster_labels, return_counts=True)\n",
    "unique_labels = unique_labels_cp.get()\n",
    "counts = counts_cp.get()\n",
    "\n",
    "num_clusters = (unique_labels != -1).sum()\n",
    "outlier_count = counts[unique_labels == -1].sum() if -1 in unique_labels else 0\n",
    "outlier_fraction = outlier_count / cluster_labels.size\n",
    "\n",
    "print(f\"Number of clusters (excluding outliers): {num_clusters}\")\n",
    "print(f\"Outliers count: {outlier_count} ({outlier_fraction:.2%} of all points)\")\n",
    "\n",
    "print(\"\\nHDBSCAN cluster label -> count\")\n",
    "for label, count in zip(unique_labels, counts):\n",
    "    print(f\"{label:>3} -> {count}\")\n",
    "\n",
    "# Compute silhouette score on subset (excluding outliers)\n",
    "sample_frac = 0.1\n",
    "num_samples_subset = min(int(sample_frac * all_latents.shape[0]), 10000)\n",
    "indices = cp.random.permutation(all_latents.shape[0])[:num_samples_subset]\n",
    "\n",
    "data_sample = all_latents[indices]\n",
    "labels_sample = cluster_labels[indices]\n",
    "mask = (labels_sample != -1)\n",
    "\n",
    "hdbscan_sil_score = silhouette_score(data_sample[mask], labels_sample[mask])\n",
    "print(f\"\\nHDBSCAN Silhouette score: {hdbscan_sil_score:.3f}\")\n",
    "\n",
    "# -------------------------\n",
    "# K-means Clustering\n",
    "# -------------------------\n",
    "\n",
    "num_clusters = 6\n",
    "kmeans_model = KMeans(num_clusters=num_clusters, n_init=\"auto\")\n",
    "kmeans_model.fit(all_latents)\n",
    "\n",
    "# Retrieve cluster labels\n",
    "kmeans_labels = kmeans_model.labels_\n",
    "\n",
    "# Check cluster counts\n",
    "unique_kmeans_labels_cp, kmeans_counts_cp = cp.unique(kmeans_labels, return_counts=True)\n",
    "unique_kmeans_labels = unique_kmeans_labels_cp.get()\n",
    "kmeans_counts = kmeans_counts_cp.get()\n",
    "\n",
    "print(f\"\\nKMeans with n_clusters={num_clusters}\")\n",
    "for label, count in zip(unique_kmeans_labels, kmeans_counts):\n",
    "    print(f\"Cluster {label} -> {count} points\")\n",
    "\n",
    "# Compute silhouette score\n",
    "kmeans_sil_score = silhouette_score(all_latents, kmeans_labels)\n",
    "print(f\"\\nKMeans Silhouette score: {kmeans_sil_score:.3f}\")"
   ],
   "id": "c57c57f0efe06464",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ---------------------------\n",
    "# Cluster Plotting Function\n",
    "# ---------------------------\n",
    "\n",
    "def plot_clusters(data_2d, labels, title=\"\", alpha_outliers=0.1, s_outliers=2, alpha_clusters=0.5, s_clusters=10):\n",
    "    # Convert to NumPy (if needed)\n",
    "    if isinstance(data_2d, cp.ndarray):\n",
    "        data_2d = cp.asnumpy(data_2d)\n",
    "    if isinstance(labels, cp.ndarray):\n",
    "        labels = cp.asnumpy(labels)\n",
    "\n",
    "    unique_labels = np.unique(labels)\n",
    "    num_clusters = np.unique(labels[labels != -1]).size\n",
    "\n",
    "    # Build a colormap for the clusters\n",
    "    colors = plt.cm.viridis_r(np.linspace(0, 1, num_clusters))\n",
    "    label_color_map = {lb: colors[i] for i, lb in\n",
    "                       enumerate(sorted(unique_labels[unique_labels != -1]))}\n",
    "    label_color_map[-1] = (0, 0, 0, 1)  # Outliers set to black\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    for lb in unique_labels:\n",
    "        mask = (labels == lb)\n",
    "\n",
    "        # Use smaller size and lower alpha for outliers\n",
    "        plot_alpha = alpha_outliers if lb == -1 else alpha_clusters\n",
    "        plot_size = s_outliers if lb == -1 else s_clusters\n",
    "\n",
    "        plt.scatter(\n",
    "            data_2d[mask, 0],\n",
    "            data_2d[mask, 1],\n",
    "            c=[label_color_map[lb]],\n",
    "            label=f\"Cluster {lb}\" if lb != -1 else \"Outliers\",\n",
    "            s=plot_size,\n",
    "            alpha=plot_alpha\n",
    "        )\n",
    "\n",
    "    plt.title(f\"{title}  (Clusters: {num_clusters})\")\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "    plt.legend(markerscale=2)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ------------------------------------\n",
    "#  t-SNE Visualization of Clusters\n",
    "# ------------------------------------\n",
    "\n",
    "cluster_labels_hdbscan = hdbscan_model.labels_\n",
    "cluster_labels_kmeans = kmeans_labels\n",
    "\n",
    "\"\"\"tsne = TSNE(\n",
    "    n_components=2,\n",
    "    perplexity=50,\n",
    "    n_neighbors=250,\n",
    "    learning_rate=20,\n",
    "    n_iter=3000,\n",
    "    verbose=False\n",
    ")\"\"\"\n",
    "\n",
    "tsne = TSNE(\n",
    "    n_components=2,\n",
    "    n_iter=3000,\n",
    "    verbose=False\n",
    ")\n",
    "data_2d_tsne = tsne.fit_transform(all_latents)\n",
    "\n",
    "#plot_clusters(data_2d_tsne, cluster_labels_hdbscan, title=\"t-SNE Visualization (HDBSCAN)\")\n",
    "plot_clusters(data_2d_tsne, cluster_labels_kmeans, title=\"t-SNE Visualization (K-means)\")\n",
    "\n",
    "# ----------------------------------\n",
    "#  UMAP Visualization of Clusters\n",
    "# ----------------------------------\n",
    "\n",
    "\"\"\"umap_2d = UMAP(\n",
    "    n_components=2,\n",
    "    n_neighbors=30,\n",
    "    learning_rate=0.5,\n",
    "    n_epochs=3000,\n",
    "    verbose=False\n",
    ")\"\"\"\n",
    "\n",
    "umap_2d = UMAP(\n",
    "    n_components=2,\n",
    "    n_neighbors=30,\n",
    "    n_epochs=3000,\n",
    "    verbose=False\n",
    ")\n",
    "data_2d_umap = umap_2d.fit_transform(all_latents)\n",
    "\n",
    "#plot_clusters(data_2d_umap, cluster_labels_hdbscan, title=\"UMAP Visualization (HDBSCAN)\")\n",
    "plot_clusters(data_2d_umap, cluster_labels_kmeans, title=\"UMAP Visualization (K-means)\")"
   ],
   "id": "d2a5555bb9646670",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -------------------------------------\n",
    "#  3-D UMAP Visualization of Clusters\n",
    "# -------------------------------------\n",
    "\n",
    "\"\"\"umap_3d = UMAP(\n",
    "    n_components=3,\n",
    "    n_neighbors=30,\n",
    "    learning_rate=0.5,\n",
    "    n_epochs=3_000,\n",
    "    verbose=False\n",
    "    )\"\"\"\n",
    "\n",
    "umap_3d = UMAP(\n",
    "    n_components=3,\n",
    "    verbose=False\n",
    "    )\n",
    "\n",
    "latent_3d_gpu = umap_3d.fit_transform(all_latents)\n",
    "\n",
    "# Send to CPU for plotting\n",
    "data_3d = cp.asnumpy(latent_3d_gpu)\n",
    "cluster_labels_np = cp.asnumpy(cluster_labels)\n",
    "\n",
    "# Create plot\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=data_3d[:, 0],\n",
    "    y=data_3d[:, 1],\n",
    "    z=data_3d[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=4,\n",
    "        color=cluster_labels_np,\n",
    "        colorscale='Viridis',\n",
    "        opacity=0.25\n",
    "    ),\n",
    "    name='Data points'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Interactive 3-D UMAP Visualization\",\n",
    "    scene=dict(\n",
    "        xaxis_title='Component 1',\n",
    "        yaxis_title='Component 2',\n",
    "        zaxis_title='Component 3'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ],
   "id": "8cc2cad3334d724c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------\n",
    "# Function for Visualizing Cluster Example Images\n",
    "# -------------------------------------------------\n",
    "\n",
    "def plot_cluster_samples(catalog, cluster_labels, cluster_id, n=9, image_col=\"file_loc\"):\n",
    "    # Grab indices\n",
    "    idxs = [i for i, lbl in enumerate(cluster_labels) if lbl == cluster_id]\n",
    "    if not idxs:\n",
    "        print(f\"No samples found for cluster {cluster_id}.\")\n",
    "        return\n",
    "\n",
    "    # Convert indices into a catalog of just that cluster\n",
    "    cluster_catalog = catalog.iloc[idxs].copy()\n",
    "\n",
    "    # Randomly sample up to n images\n",
    "    if len(cluster_catalog) > n:\n",
    "        cluster_catalog = cluster_catalog.sample(n)\n",
    "    else:\n",
    "        cluster_catalog = cluster_catalog.sample(frac=1.0)\n",
    "\n",
    "    # Initialize layout\n",
    "    num_samples = len(cluster_catalog)\n",
    "    rows = int(math.ceil(num_samples ** 0.5))\n",
    "    cols = int(math.ceil(num_samples / rows))\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(2.5 * cols, 2.5 * rows))\n",
    "    if rows * cols == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "\n",
    "    # Plot each sample\n",
    "    for ax, (_, row) in zip(axes, cluster_catalog.iterrows()):\n",
    "        file_path = row[image_col]\n",
    "        try:\n",
    "            with Image.open(file_path) as img:\n",
    "                ax.imshow(img)\n",
    "            ax.set_title(f\"Cluster {cluster_id}\", fontsize=9)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading '{file_path}': {e}\")\n",
    "        ax.axis(\"off\")\n",
    "    for leftover_ax in axes[num_samples:]:\n",
    "        leftover_ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# Visualizing Cluster Examples\n",
    "# -------------------------------\n",
    "\n",
    "for cluster_id in range(7):\n",
    "    print(f\"\\nShowing samples from cluster {cluster_id}...\")\n",
    "    plot_cluster_samples(\n",
    "        catalog=og_test_catalog,\n",
    "        cluster_labels=cluster_labels_hdbscan,\n",
    "        cluster_id=cluster_id,\n",
    "        n=9,\n",
    "        image_col=\"file_loc\"\n",
    "    )"
   ],
   "id": "a200abc47ba94d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "783b645a29c3ce4b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
